{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 1) get the individual information  -->\n",
    "<!-- 'Patient ID', 'EMU ID', 'Filename': filename,'Spike Time (s)': 'Side': spike side, 'YASA': yasa_stage -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, Manager # Import Pool and Manager for multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Input folders\n",
    "pkl_roots = [\n",
    "    '/mnt/sauce/littlab/users/jurikim/ied_yesno/colab/Jack/add_144/pickle_outputs/', #pickle data(SN2, SN2_zero_left, SN2_zero_right, YASA) for the 2017-2022 data\n",
    "    # '/mnt/sauce/littlab/users/jurikim/ied_yesno/colab/Jack/add_144/pickle_outputs_2324_updates/' #pickle data(SN2, SN2_zero_left, SN2_zero_right, YASA) for the 2023-2024 data\n",
    "]\n",
    "deid_csv = '/mnt/sauce/littlab/users/jurikim/ied_yesno/add_2023/deid_with_redcap_Apr3025.csv' #deidentified_table from Erin\n",
    "output_csv = '/mnt/sauce/littlab/users/jurikim/ied_yesno/colab/Jack/add_144/outputs/spike_summary_1.csv'\n",
    "\n",
    "# Parameters\n",
    "THRESHOLD = 0.43\n",
    "SAMPLES_PER_SEC = int(1 / 0.0625)  # 16\n",
    "SKIP_FRONT = 8\n",
    "SKIP_BACK = 9\n",
    "NUM_PROCESSES = 16 # Define the number of CPU cores to use\n",
    "\n",
    "# Load de-identified mapping\n",
    "deid_df = pd.read_csv(deid_csv)\n",
    "deid_df['admission_id'] = deid_df['admission_id'].astype(str)\n",
    "# Dictionary is safe to use in multiprocessing as it's read-only\n",
    "ADMISSION_TO_PATIENT = dict(zip(deid_df['admission_id'], deid_df['patient_id']))\n",
    "\n",
    "# Spike clustering function (No change)\n",
    "def cluster_spikes_with_max(SN2, SN2_right, SN2_left, threshold):\n",
    "    spike_info = []\n",
    "    i = SAMPLES_PER_SEC // 2\n",
    "    while i + SAMPLES_PER_SEC // 2 < len(SN2):\n",
    "        if SN2[i] > threshold:\n",
    "            win_start = max(0, i - SAMPLES_PER_SEC // 2)\n",
    "            win_end = min(len(SN2), i + SAMPLES_PER_SEC // 2)\n",
    "            max_left = np.max(SN2_left[win_start:win_end])\n",
    "            max_right = np.max(SN2_right[win_start:win_end])\n",
    "            side = 'R' if max_right > max_left else 'L'\n",
    "            spike_info.append((i, side))\n",
    "            i += SAMPLES_PER_SEC\n",
    "        else:\n",
    "            i += 1\n",
    "    return spike_info\n",
    "\n",
    "# Worker function to process a single PKL file\n",
    "def process_pkl_file(pkl_path):\n",
    "    \"\"\"Processes a single PKL file and returns a list of dictionaries (rows).\"\"\"\n",
    "    \n",
    "    filename = os.path.basename(pkl_path)\n",
    "    \n",
    "    match_emu = re.match(r'(EMU\\d+)', filename)\n",
    "    if not match_emu:\n",
    "        print(f\"[SKIP] Cannot extract EMU ID from {filename}\")\n",
    "        return None\n",
    "    emu_id = match_emu.group(1)\n",
    "    \n",
    "    patient_id = ADMISSION_TO_PATIENT.get(emu_id, 'UNKNOWN')\n",
    "    \n",
    "    local_rows = []\n",
    "\n",
    "    try:\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        if not all(k in data for k in ['SN2', 'SN2_zero_left', 'SN2_zero_right', 'YASA']):\n",
    "            return None\n",
    "\n",
    "        # Apply skip parameters\n",
    "        SN2 = data['SN2'][SKIP_FRONT:-SKIP_BACK]\n",
    "        SN2_right = data['SN2_zero_left'][SKIP_FRONT:-SKIP_BACK]\n",
    "        SN2_left = data['SN2_zero_right'][SKIP_FRONT:-SKIP_BACK]\n",
    "        YASA = data['YASA'][SKIP_FRONT:-SKIP_BACK]\n",
    "\n",
    "        spike_data = cluster_spikes_with_max(SN2, SN2_right, SN2_left, THRESHOLD)\n",
    "\n",
    "        for idx, side in spike_data:\n",
    "            yasa_stage = YASA[idx] if idx < len(YASA) else np.nan\n",
    "            \n",
    "            local_rows.append({\n",
    "                'Patient ID': patient_id,\n",
    "                'EMU ID': emu_id, \n",
    "                'Filename': filename,\n",
    "                'Spike Time (s)': round((idx + SKIP_FRONT) * 0.0625, 2),\n",
    "                'Side': side,\n",
    "                'YASA': yasa_stage\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"[ERROR] {filename}: {e}\") # Suppress excessive printing from workers\n",
    "        return None\n",
    "\n",
    "    return local_rows\n",
    "\n",
    "# Sort by EMU ID, day, etc. (No change)\n",
    "def extract_sort_key(path):\n",
    "    fname = os.path.basename(path)\n",
    "    match = re.search(r'(EMU\\d+)_(?:Day(\\d+)_)?(\\d+)_(\\d+\\.\\d+)_\\d+\\.\\d+\\.pkl$', fname)\n",
    "    if match:\n",
    "        emu_id = match.group(1)\n",
    "        day = int(match.group(2)) if match.group(2) else 0\n",
    "        clip = int(match.group(3))\n",
    "        start_sec = float(match.group(4))\n",
    "        return (emu_id, day, clip, start_sec)\n",
    "    return ('ZZZ', 999, 999, float('inf'))\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION BLOCK ---\n",
    "if __name__ == '__main__':\n",
    "    # --- PKL File Path Collection ---\n",
    "    all_pkl_paths = []\n",
    "    print(\"ðŸ” Collecting PKL file paths from both root directories...\")\n",
    "    for pkl_root in pkl_roots:\n",
    "        if not os.path.isdir(pkl_root):\n",
    "            print(f\"[WARNING] Directory does not exist. Skipping: {pkl_root}\")\n",
    "            continue\n",
    "            \n",
    "        for root, _, files in os.walk(pkl_root):\n",
    "            for file in files:\n",
    "                if file.endswith('.pkl'):\n",
    "                    all_pkl_paths.append(os.path.join(root, file))\n",
    "\n",
    "    print(f\" Collected a total of {len(all_pkl_paths)} PKL files.\")\n",
    "\n",
    "    # Sort files to ensure deterministic spike numbering later (optional but good practice)\n",
    "    all_pkl_paths.sort(key=extract_sort_key)\n",
    "    \n",
    "    # --- Multiprocessing Pool Execution ---\n",
    "    print(f\" Starting spike extraction using {NUM_PROCESSES} CPU cores...\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    with Pool(processes=NUM_PROCESSES) as pool:\n",
    "        # pool.map returns results in the order the inputs were submitted.\n",
    "        # We pass the list of pkl paths to the worker function.\n",
    "        results = pool.map(process_pkl_file, all_pkl_paths)\n",
    "\n",
    "    # --- Combine Results and Finalize Spike Numbering ---\n",
    "    \n",
    "    all_rows = []\n",
    "    spike_counters = {}\n",
    "    \n",
    "    # Flatten the list of lists returned by pool.map\n",
    "    for result_list in results:\n",
    "        if result_list:\n",
    "            all_rows.extend(result_list)\n",
    "\n",
    "    print(f\"Gathered {len(all_rows)} raw spike entries.\")\n",
    "\n",
    "    # Re-sort by EMU ID, then Filename, then Spike Time to ensure correct spike numbering\n",
    "    if all_rows:\n",
    "        df_raw = pd.DataFrame(all_rows)\n",
    "        # Sort order: EMU ID > Filename > Spike Time (s)\n",
    "        df_raw = df_raw.sort_values(by=['EMU ID', 'Filename', 'Spike Time (s)'])\n",
    "        \n",
    "        # Apply spike numbering sequentially within each EMU ID\n",
    "        for emu_id, group in df_raw.groupby('EMU ID'):\n",
    "            if emu_id not in spike_counters:\n",
    "                spike_counters[emu_id] = 1\n",
    "                \n",
    "            group_size = len(group)\n",
    "            group['Spike #'] = range(spike_counters[emu_id], spike_counters[emu_id] + group_size)\n",
    "            spike_counters[emu_id] += group_size\n",
    "            \n",
    "            # Update the main list of rows with the new spike numbering\n",
    "            all_rows.extend(group.to_dict('records'))\n",
    "\n",
    "        # Prepare final DataFrame for saving\n",
    "        df = pd.DataFrame(all_rows)\n",
    "        # Drop the temporary 'EMU ID' column used for grouping/sorting in the worker process\n",
    "        df = df.drop(columns=['EMU ID'], errors='ignore')\n",
    "        \n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    print(f\"\\nâœ¨ Total number of extracted spike data points: {len(df)}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    try:\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\" Results successfully saved to '{output_csv}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] CSV save failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 2) summary of each patient's # spike, laterality, and YASA -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add duration in the outcome\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Input/output paths\n",
    "input_csv = '/mnt/sauce/littlab/users/jurikim/ied_yesno/colab/Jack/add_144/outputs/spike_summary_1.csv'\n",
    "output_csv = '/mnt/sauce/littlab/users/jurikim/ied_yesno/colab/Jack/add_144/outputs/spike_summary_2.csv'\n",
    "pkl_base_dir = '/mnt/sauce/littlab/users/jurikim/ied_yesno/colab/Jack/add_144/pickle_outputs/'\n",
    "\n",
    "# Load input CSV\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Clean and prepare data\n",
    "df['admission_id'] = df['Filename'].str.extract(r'(EMU\\d+)')\n",
    "df['Spike #'] = df['Spike #'].astype(int)\n",
    "df['Side'] = df['Side'].fillna('')\n",
    "df['YASA'] = df['YASA'].fillna('')\n",
    "\n",
    "# Define sleep stages\n",
    "sleep_stages = ['N1', 'N2', 'N3', 'R']\n",
    "\n",
    "# Initialize list for rows\n",
    "summary_rows = []\n",
    "\n",
    "# Group by patient\n",
    "for patient_id, group in df.groupby('Patient ID'):\n",
    "    admission_ids = sorted(group['admission_id'].unique())\n",
    "    admission_id_str = ','.join(admission_ids)\n",
    "\n",
    "    # Spike counts\n",
    "    total_spikes = len(group)\n",
    "    left_spikes = (group['Side'] == 'L').sum()\n",
    "    right_spikes = (group['Side'] == 'R').sum()\n",
    "    wake_spikes = (group['YASA'] == 'W').sum()\n",
    "    sleep_spikes = group['YASA'].isin(sleep_stages).sum()\n",
    "\n",
    "    # Duration counts (across all EMUxxxx folders)\n",
    "    total_duration = 0\n",
    "    wake_duration = 0\n",
    "    sleep_duration = 0\n",
    "\n",
    "    for adm_id in admission_ids:\n",
    "        pkl_dir = os.path.join(pkl_base_dir, adm_id)\n",
    "        if os.path.exists(pkl_dir):\n",
    "            for filename in os.listdir(pkl_dir):\n",
    "                if filename.endswith('.pkl'):\n",
    "                    pkl_path = os.path.join(pkl_dir, filename)\n",
    "                    try:\n",
    "                        with open(pkl_path, 'rb') as f:\n",
    "                            data = pickle.load(f)\n",
    "                            yasa = np.array(data.get('YASA', []))\n",
    "                            time = np.array(data.get('Time', []))\n",
    "\n",
    "                            if len(time) > 0:\n",
    "                                total_duration += time[-1]  # last timestamp in seconds\n",
    "\n",
    "                            wake_duration += (yasa == 'W').sum() * 0.0625\n",
    "                            sleep_duration += sum((yasa == stage).sum() for stage in sleep_stages) * 0.0625\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not load {pkl_path}: {e}\")\n",
    "\n",
    "\n",
    "    # Append summary for this patient\n",
    "    summary_rows.append({\n",
    "        'patient_id': patient_id,\n",
    "        'admission_id': admission_id_str,\n",
    "        'Total_spikes': total_spikes,\n",
    "        'Left_spikes': left_spikes,\n",
    "        'Right_spikes': right_spikes,\n",
    "        'Wake_spikes': wake_spikes,\n",
    "        'Sleep_spikes': sleep_spikes,\n",
    "        'Total_duration(sec)': total_duration,\n",
    "        'Wake_duration(sec)': wake_duration,\n",
    "        'Sleep_duration(sec)': sleep_duration\n",
    "    })\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Save to CSV\n",
    "summary_df.to_csv(output_csv, index=False)\n",
    "print(f\"Spike summary saved to: {output_csv}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (erin_env)",
   "language": "python",
   "name": "erin_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
